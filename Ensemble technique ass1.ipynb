{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27108a71-6757-4d60-bbfa-9609c58e9dc7",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73ad07-069b-45ce-8172-b186411990b7",
   "metadata": {},
   "source": [
    "In machine learning, ensemble techniques involve combining the predictions of multiple models to improve overall performance, accuracy, and robustness. The idea is that by aggregating the results of several models, the ensemble can capture different patterns or errors, reducing the likelihood of overfitting and improving generalization on new data.\n",
    "\n",
    "### Types of Ensemble Techniques:\n",
    "\n",
    "1. Bagging (Bootstrap Aggregating):\n",
    "\n",
    "Models are trained on different subsets of the training data, usually generated by bootstrapping (random sampling with replacement).\n",
    "\n",
    "Example: Random Forest, which is an ensemble of decision trees trained on different subsets of the data.\n",
    "\n",
    "2. Boosting:\n",
    "\n",
    "Models are trained sequentially, with each subsequent model focusing on correcting the errors of the previous models.\n",
    "\n",
    "Example: AdaBoost, Gradient Boosting Machines (GBM), XGBoost.\n",
    "\n",
    "3. Stacking:\n",
    "\n",
    "Multiple models (often of different types) are trained, and their predictions are combined using another model (meta-learner) to make the final prediction.\n",
    "\n",
    "Example: Stacked Generalization, where the base models feed into a meta-model.\n",
    "\n",
    "4. Voting:\n",
    "\n",
    "The predictions from different models are combined using majority voting (for classification) or averaging (for regression).\n",
    "\n",
    "Example: A Voting Classifier that aggregates the predictions from models like Logistic Regression, SVM, and Decision Trees.\n",
    "\n",
    "### Benefits of Ensemble Techniques:\n",
    "\n",
    "Improved accuracy: By combining models, ensembles often outperform individual models.\n",
    "\n",
    "Reduced overfitting: Since ensemble methods rely on multiple models, they tend to be less likely to overfit to the training data.\n",
    "\n",
    "Better generalization: Ensembles can better capture diverse patterns in data, leading to improved generalization to unseen data.\n",
    "\n",
    "### Drawbacks:\n",
    "\n",
    "Increased computational cost: Training multiple models can be resource-intensive.\n",
    "\n",
    "Complexity: Understanding and maintaining ensemble models can be more complex than single models.\n",
    "\n",
    "Ensemble methods are widely used in both classification and regression tasks, especially in competitions like Kaggle, where they often provide state-of-the-art results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e057c9-529e-4bfc-97af-faa26581da8a",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029b548-7f6a-490d-85ca-8843ba3d6af5",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several key reasons, all centered around improving model performance, robustness, and accuracy. Here's why ensemble methods are widely employed:\n",
    "\n",
    "1. Improved Accuracy:\n",
    "Combining multiple models: Ensemble methods leverage the strengths of different models by combining their predictions, which often leads to better overall performance. Individual models might struggle to capture all the complexities in the data, but their combined output can often outperform any single model.\n",
    "\n",
    "Example: In Random Forests, many decision trees are trained, and their average prediction provides a more accurate result.\n",
    "\n",
    "2. Reduced Overfitting:\n",
    "More generalizable models: Individual models can overfit, especially complex models that memorize training data instead of learning patterns. By averaging or combining multiple models, ensemble techniques tend to smooth out extreme predictions and overfitting tendencies, improving the model's ability to generalize to new data.\n",
    "\n",
    "Example: Boosting algorithms like AdaBoost reduce overfitting by focusing on correcting the mistakes of prior models iteratively.\n",
    "\n",
    "3. Model Robustness:\n",
    "Mitigating weaknesses of individual models: Different models might make errors on different parts of the data. When combined, the models can complement each other, reducing the overall error. This leads to a more robust model that is less sensitive to the specific weaknesses of any one algorithm.\n",
    "\n",
    "Example: Bagging in Random Forests reduces the variance of individual decision trees, making the overall model more stable.\n",
    "\n",
    "4. Handling Bias-Variance Trade-off:\n",
    "Reducing variance: High-variance models (e.g., decision trees) tend to overfit, while high-bias models (e.g., linear models) tend to underfit. Ensemble methods like bagging reduce the variance of high-variance models by averaging predictions, while boosting reduces the bias by iteratively improving the model.\n",
    "\n",
    "Example: Gradient Boosting reduces both bias and variance by combining weak learners sequentially to minimize error.\n",
    "\n",
    "5. Combining Weak Learners:\n",
    "Creating strong models from weak learners: Boosting techniques such as AdaBoost and Gradient Boosting can combine weak learners (models that are only slightly better than random guessing) to build a strong model that makes more accurate predictions.\n",
    "\n",
    "Example: In AdaBoost, weak classifiers are trained in sequence, and each subsequent classifier focuses on the mistakes made by the previous ones.\n",
    "\n",
    "6. Improved Stability and Generalization:\n",
    "Less susceptible to data noise: Individual models can be sensitive to noise in the training data. By aggregating multiple models, ensemble techniques tend to \"average out\" noise effects, resulting in more stable predictions.\n",
    "\n",
    "Example: Bagging is often used to reduce the impact of noise in the data by training models on different bootstrapped samples.\n",
    "\n",
    "7. Versatility:\n",
    "Flexible across different algorithms: Ensemble techniques can be applied across a wide range of machine learning algorithms (e.g., decision trees, SVMs, neural networks) and combined in different ways (bagging, boosting, stacking, etc.), making them versatile tools for improving performance in a variety of tasks.\n",
    "\n",
    "Example: Stacking allows for the combination of different types of models (e.g., decision trees, logistic regression, and neural networks) to build a meta-model.\n",
    "\n",
    "8. Participation in Competitions:\n",
    "Better competition results: Ensemble techniques are commonly used in machine learning competitions (e.g., Kaggle) because they consistently provide better results by combining the strengths of various models.\n",
    "\n",
    "In summary, ensemble techniques are employed in machine learning to enhance prediction accuracy, reduce overfitting, increase robustness, and improve generalization. By leveraging multiple models, they offer a powerful approach to address the limitations of individual models and improve overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765c7fd-006d-49af-8fa9-0e3afd8d1482",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ad14b-f386-41c0-b2b6-e91d2a149628",
   "metadata": {},
   "source": [
    "Bagging (short for Bootstrap Aggregating) is an ensemble technique in machine learning that aims to improve the accuracy and stability of models by reducing variance and overfitting. It works by training multiple versions of a model on different subsets of the data, then aggregating their predictions to produce a more reliable and accurate final output.\n",
    "\n",
    "### How Bagging Works:\n",
    "\n",
    "1. Bootstrap Sampling:\n",
    "\n",
    "Multiple datasets are created by drawing random samples with replacement from the original training set. This means that some data points may appear more than once in a subset, while others may not appear at all.\n",
    "These randomly created datasets are called bootstrap samples.\n",
    "\n",
    "2. Train Multiple Models:\n",
    "\n",
    "A model (e.g., decision tree) is trained independently on each bootstrap sample. Because the datasets are slightly different, the models will learn slightly different patterns from the data.\n",
    "\n",
    "3. Aggregate the Predictions:\n",
    "\n",
    "For classification tasks, the final prediction is determined by a majority vote (i.e., the class predicted most often by the individual models).\n",
    "For regression tasks, the predictions of the individual models are averaged to get the final output.\n",
    "\n",
    "### Key Benefits of Bagging:\n",
    "\n",
    "1. Reduced Variance:\n",
    "\n",
    "Bagging helps to reduce the variance of high-variance models like decision trees. By averaging or voting over many models, it reduces the likelihood of overfitting on specific parts of the training data.\n",
    "\n",
    "2. Improved Accuracy:\n",
    "\n",
    "By combining multiple models, bagging often provides better accuracy than a single model, especially in cases where the base model is prone to overfitting.\n",
    "\n",
    "3. Increased Robustness:\n",
    "\n",
    "Because bagging generates multiple models on different subsets of data, the final aggregated model is more robust and less sensitive to small changes or noise in the training data.\n",
    "\n",
    "Example of Bagging: Random Forest\n",
    "\n",
    "Random Forest is a popular algorithm that uses bagging as its core principle. In Random Forest, multiple decision trees are trained on different bootstrap samples of the data, and a random subset of features is chosen for each split in the trees. The final prediction is determined by averaging (in regression) or majority voting (in classification) across all the trees.\n",
    "\n",
    "### When to Use Bagging:\n",
    "\n",
    "Bagging is particularly effective for high-variance models like decision trees.\n",
    "It is useful when you want to reduce overfitting and improve the stability of the model, especially on noisy datasets.\n",
    "\n",
    "### Limitations of Bagging:\n",
    "While it reduces variance, it doesn’t address high bias. For models with high bias (e.g., linear models), bagging may not improve performance.\n",
    "\n",
    "It increases computational complexity as multiple models need to be trained.\n",
    "\n",
    "In summary, bagging is a powerful ensemble technique that helps reduce variance and improve prediction accuracy by training multiple models on different bootstrap samples of the data and aggregating their results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d286e46-ad5d-4248-8ce2-8581b087c653",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a256c-26db-47cc-b4cb-49736526ec00",
   "metadata": {},
   "source": [
    "Boosting is an ensemble technique in machine learning that aims to improve model accuracy by converting weak learners (models that perform slightly better than random guessing) into strong learners. It does so by training models sequentially, where each subsequent model focuses on correcting the errors made by the previous models. Unlike bagging, which works in parallel, boosting is a sequential process that gives higher importance to the instances that are harder to predict.\n",
    "\n",
    "### How Boosting Works:\n",
    "\n",
    "1. Initial Model:\n",
    "A weak learner (usually a simple model like a decision tree with limited depth) is trained on the original training dataset.\n",
    "\n",
    "2. Adjusting Weights/Errors:\n",
    "After training the initial model, the errors (misclassified or poorly predicted instances) are identified. In the next iteration, the model gives more weight or emphasis to these difficult cases. This forces the next model to focus more on the hard-to-predict instances.\n",
    "\n",
    "3. Sequential Learning:\n",
    "A new weak learner is added in each subsequent round, trained on the dataset with updated weights/errors.\n",
    "The process continues for a predefined number of iterations or until the error cannot be further reduced.\n",
    "\n",
    "4. Final Prediction:\n",
    "In classification tasks, the final prediction is made by combining the weighted predictions of all models (using a weighted vote). For regression tasks, the final output is a weighted sum of the predictions.\n",
    "\n",
    "### Key Characteristics of Boosting:\n",
    "\n",
    "1. Sequential Training: Each new model is trained to improve the performance of the previous models by correcting their errors.\n",
    "\n",
    "2. Focus on Errors: The models are not trained equally; they are designed to give more attention to the instances that previous models struggled with.\n",
    "3. Adaptive: Boosting adaptively modifies the model by concentrating more on the problematic parts of the data.\n",
    "\n",
    "### Popular Boosting Algorithms:\n",
    "\n",
    "1. AdaBoost (Adaptive Boosting):\n",
    "\n",
    "AdaBoost assigns higher weights to misclassified instances and lowers the weights of correctly classified instances. The subsequent model focuses more on the misclassified instances.\n",
    "Final prediction is a weighted majority vote (for classification) or a weighted sum (for regression).\n",
    "\n",
    "2. Gradient Boosting:\n",
    "\n",
    "In Gradient Boosting, models are added sequentially to minimize the residual errors (differences between the actual and predicted values). The process involves fitting each new model to the residuals of the previous model.\n",
    "This method is highly effective for regression and classification tasks.\n",
    "\n",
    "#### XGBoost (Extreme Gradient Boosting):\n",
    "\n",
    "XGBoost is an optimized and scalable version of Gradient Boosting that includes regularization techniques to avoid overfitting, faster computation using parallelization, and other improvements.\n",
    "It’s widely used in machine learning competitions due to its high performance and efficiency.\n",
    "\n",
    "#### LightGBM:\n",
    "\n",
    "LightGBM is another variant of Gradient Boosting that focuses on handling large datasets with faster training times by using a leaf-wise growth strategy.\n",
    "\n",
    "#### CatBoost:\n",
    "\n",
    "CatBoost is designed to handle categorical features without the need for extensive preprocessing, making it very efficient for specific datasets.\n",
    "\n",
    "### Benefits of Boosting:\n",
    "\n",
    "1. High Accuracy: Boosting often yields models with very high accuracy, as it focuses on correcting the mistakes of previous models.\n",
    "\n",
    "2. Reduced Bias: Boosting can significantly reduce the bias of weak learners, making the overall model more accurate.\n",
    "\n",
    "3. Adaptive: Since boosting focuses on the hard-to-predict examples, it adapts to the complexity of the data, often outperforming other algorithms.\n",
    "\n",
    "### Limitations of Boosting:\n",
    "\n",
    "1. Prone to Overfitting: Boosting, especially with too many iterations or weak learners, can lead to overfitting if not carefully regularized.\n",
    "2. Computationally Expensive: Sequential training can be slow and computationally intensive compared to parallel techniques like bagging.\n",
    "3. Sensitive to Noisy Data: Since boosting places more emphasis on hard-to-predict cases, noisy data points or outliers may receive too much attention, potentially degrading performance.\n",
    "\n",
    "### When to Use Boosting:\n",
    "\n",
    "1. When accuracy is critical: Boosting is often used in scenarios where high predictive accuracy is required, such as in competitions or for complex datasets.\n",
    "2. For weak models: It is effective when the base model is a weak learner (e.g., shallow decision trees).\n",
    "3. When handling imbalance or difficult-to-predict data: Boosting shines in situations where some parts of the data are harder to predict, as it can adapt and focus on those examples.\n",
    "\n",
    "\n",
    "In summary, boosting is a powerful ensemble technique that combines weak learners sequentially, with each new model focusing on correcting the errors made by the previous models. By adaptively giving more weight to misclassified instances, boosting reduces bias and improves accuracy, but care must be taken to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57d9b8-559c-45c5-bcad-76b2f7d64f60",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0236264d-379c-436a-9ef9-9a7d4c89ee95",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several key benefits in machine learning by combining the strengths of multiple models to improve predictive performance, stability, and robustness. Here are the primary advantages of using ensemble methods:\n",
    "\n",
    "1. Improved Accuracy\n",
    "\n",
    "Combining predictions from multiple models often leads to better accuracy than using a single model. By aggregating the outputs, ensemble methods can capture more complex patterns and reduce the overall error.\n",
    "\n",
    "Example: In a Random Forest, multiple decision trees are averaged, providing better accuracy than a single decision tree.\n",
    "\n",
    "2. Reduced Overfitting\n",
    "\n",
    "Ensemble methods help to reduce overfitting, especially when using high-variance models like decision trees. By averaging predictions from multiple models, ensemble techniques smooth out the predictions and make the model more generalizable to unseen data.\n",
    "\n",
    "Example: Bagging techniques, such as Random Forest, help reduce overfitting by training on bootstrapped samples of data.\n",
    "\n",
    "3. Increased Robustness\n",
    "By aggregating different models, ensemble methods create more robust predictions that are less sensitive to noise and outliers in the data. This ensures the model performs well even in the presence of data variations or noisy inputs.\n",
    "\n",
    "Example: Boosting algorithms iteratively focus on correcting errors, which helps create a more robust model over time.\n",
    "\n",
    "4. Handling Bias-Variance Trade-off\n",
    "Ensemble methods help to address the bias-variance trade-off. Bagging reduces the variance of high-variance models (like decision trees), while boosting helps reduce both bias and variance by iteratively improving weak models.\n",
    "\n",
    "Example: Gradient Boosting Machines (GBMs) help reduce bias by adding models sequentially and improving upon the errors of previous ones.\n",
    "\n",
    "5. Better Generalization\n",
    "By combining predictions from multiple models, ensembles often lead to better generalization to new, unseen data. This means that the ensemble model is less likely to memorize the training data and more likely to perform well on test data.\n",
    "\n",
    "Example: A Voting Classifier that averages the predictions from models like Logistic Regression, SVM, and Decision Trees can generalize better by utilizing the diverse perspectives of each model.\n",
    "\n",
    "6. Resilience to Different Types of Errors\n",
    "Different models may make different types of errors, and combining them can help offset these weaknesses. If one model underperforms on a certain type of data, the other models can compensate, leading to more consistent predictions.\n",
    "\n",
    "Example: In stacking, multiple base models (e.g., decision trees, linear models, neural networks) are trained, and their outputs are combined by a meta-learner, which helps account for varying model errors.\n",
    "\n",
    "7. Flexibility Across Algorithms\n",
    "Ensemble methods can be applied across different types of machine learning models (e.g., decision trees, neural networks, SVMs), making them highly versatile. You can combine models with diverse strengths and weaknesses to create a powerful overall model.\n",
    "\n",
    "Example: Stacking allows you to mix models of different types, combining their predictions for greater overall accuracy.\n",
    "\n",
    "8. Better Performance in Competitions\n",
    "In machine learning competitions like Kaggle, ensemble techniques are widely used and often provide state-of-the-art results. The best-performing solutions usually involve some form of ensemble learning due to its ability to produce highly accurate and generalized models.\n",
    "\n",
    "9. Reduction of Noise Effects\n",
    "Ensembles reduce the impact of noise in the data by averaging or combining predictions from multiple models. This means that models trained on noisy data points can cancel out their errors when aggregated with other models.\n",
    "\n",
    "Example: Bagging helps reduce the noise effect by generating different bootstrapped samples, so models trained on noisy data points don't dominate the final prediction.\n",
    "\n",
    "10. Improved Stability\n",
    "Ensemble techniques lead to more stable predictions because they mitigate the risk of depending on the peculiarities of one particular model or dataset. The diversity among the models ensures that the ensemble is less sensitive to specific data fluctuations.\n",
    "\n",
    "Example: Random Forest increases stability by training multiple trees on random subsets of data, ensuring that no single tree has an outsized influence on the final prediction.\n",
    "\n",
    "11. Handling Imbalanced Data\n",
    "Boosting methods like AdaBoost and Gradient Boosting are particularly useful for handling imbalanced datasets. They focus on hard-to-predict or misclassified examples, ensuring that minority classes receive enough attention.\n",
    "\n",
    "Example: In an imbalanced dataset, boosting focuses on the misclassified examples, including those from the minority class, improving overall performance.\n",
    "\n",
    "12. Versatility and Scalability\n",
    "Ensemble techniques are versatile and can be used in a variety of settings, including classification, regression, and ranking tasks. Additionally, methods like XGBoost and LightGBM are designed for efficiency, making them scalable to large datasets.\n",
    "\n",
    "Example: XGBoost is widely used in industry and competitions because of its scalability and efficiency with large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d143b-94f6-4873-a3f6-09046b3b2d57",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e3588-aede-42b2-b513-b25cd3076b9b",
   "metadata": {},
   "source": [
    "Ensemble techniques are not always better than individual models. They often provide superior accuracy and generalization, particularly for complex and noisy datasets, but there are cases where individual models may be more appropriate due to simplicity, interpretability, or computational efficiency. It’s important to assess the specific problem, dataset characteristics, and application requirements to determine whether an ensemble is the best approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec946a25-61a3-4ccf-9691-3120088ed8fc",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77841754-8b7e-47a3-8c65-78b68e0bd912",
   "metadata": {},
   "source": [
    "The bootstrap method is a resampling technique used to estimate the distribution of a statistic (e.g., mean, median, or standard deviation) and to calculate confidence intervals without making strong assumptions about the data's distribution. Here's how the confidence interval is calculated using bootstrap:\n",
    "\n",
    "### Steps to Calculate Confidence Interval Using Bootstrap:\n",
    "1. Generate Bootstrap Samples:\n",
    "\n",
    "From the original dataset of size n, generate multiple bootstrap samples by sampling with replacement. Each bootstrap sample has the same size n as the original dataset, but since sampling is done with replacement, some data points will be repeated, and others may not appear at all.\n",
    "\n",
    "Suppose you generate B bootstrap samples (usually several thousand, e.g., B=1000).\n",
    "\n",
    "2. Compute the Statistic of Interest for Each Sample:\n",
    "\n",
    "For each bootstrap sample, compute the statistic of interest, such as the mean, median, or another parameter. Let’s say the statistic is denoted as 𝜃^\n",
    "\n",
    "This will give you B estimates of the statistic 𝜃^1,𝜃^2,...,𝜃^𝐵\n",
    "\n",
    "3. Sort the Bootstrap Statistics:\n",
    "\n",
    "Sort the B bootstrap estimates 𝜃^1,𝜃^2,...,𝜃^𝐵 in ascending order.\n",
    "\n",
    "4. Determine the Percentile-Based Confidence Interval:\n",
    "\n",
    "To construct a confidence interval using the percentile method, you select the lower and upper percentiles from the sorted bootstrap estimates.\n",
    "\n",
    "For a 95% confidence interval, use the 2.5th percentile and the 97.5th percentile of the bootstrap distribution:\n",
    "\n",
    "Lower bound: The value at the 2.5th percentile of the sorted bootstrap statistics.\n",
    "\n",
    "Upper bound: The value at the 97.5th percentile of the sorted bootstrap statistics.\n",
    "\n",
    "This range forms the confidence interval for the statistic.\n",
    "\n",
    "### Example of Bootstrap Confidence Interval Calculation:\n",
    "Let’s assume we have a dataset of 100 values, and we want to estimate the 95% confidence interval for the mean using bootstrap.\n",
    "\n",
    "Step 1: Generate 1000 bootstrap samples by resampling the data with replacement.\n",
    "\n",
    "Step 2: Compute the mean for each of the 1000 bootstrap samples.\n",
    "\n",
    "Step 3: Sort the 1000 bootstrap means in ascending order.\n",
    "\n",
    "Step 4: Identify the 2.5th percentile and the 97.5th percentile of the sorted bootstrap means. These values will be the lower and upper bounds of the 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8bfb9-98ce-4198-9aae-452e56ff34c7",
   "metadata": {},
   "source": [
    "Q8. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "654fc4b8-efc5-4d00-8351-acb5b8d17da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.033849846852862, 15.061040878849226)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "sample_mean = 15  # mean height of sample\n",
    "sample_std = 2    # standard deviation of sample\n",
    "n = 50            # sample size\n",
    "num_bootstrap = 10000  # number of bootstrap samples\n",
    "confidence_level = 95\n",
    "\n",
    "# Simulating the original sample (assuming normal distribution)\n",
    "np.random.seed(42)\n",
    "original_sample = np.random.normal(sample_mean, sample_std, n)\n",
    "\n",
    "# Bootstrap process\n",
    "bootstrap_means = []\n",
    "for _ in range(num_bootstrap):\n",
    "    # Resample the data with replacement\n",
    "    bootstrap_sample = np.random.choice(original_sample, size=n, replace=True)\n",
    "    # Calculate the mean of the bootstrap sample\n",
    "    bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "\n",
    "# Convert bootstrap means to numpy array for sorting\n",
    "bootstrap_means = np.array(bootstrap_means)\n",
    "\n",
    "# Calculate the confidence interval\n",
    "lower_percentile = (100 - confidence_level) / 2\n",
    "upper_percentile = confidence_level + lower_percentile\n",
    "\n",
    "lower_bound = np.percentile(bootstrap_means, lower_percentile)\n",
    "upper_bound = np.percentile(bootstrap_means, upper_percentile)\n",
    "\n",
    "lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f688c-0720-4792-af39-ae362f46cad6",
   "metadata": {},
   "source": [
    "The 95% confidence interval for the population mean height, estimated using the bootstrap method, is approximately [14.03 meters, 15.06 meters]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84069573-a5a3-4b6f-8f8c-9a8fe529457c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
